{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e57f81-f6d5-4426-b016-22e0da419aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/conda_env/chr_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import glob\n",
    "import collections\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,GroupKFold\n",
    "import torch\n",
    "# pip install prefetch_generator\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from  transformers import AdamW, AutoTokenizer,AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results\n",
    "    Arguments:\n",
    "        seed {int} -- Number of the seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "SEED=2020\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceca1a1-5f32-4113-90da-e8db64791e08",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c729a2f7-7c95-4206-917b-e974ac1f7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "BERT_PATH = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MODEL_PATH = \"./save/recall/2023_recall_v1_add_text_nice_valid.pkl\"\n",
    "PROMPT_LEN = 512\n",
    "WIKI_LEN = 512\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 128\n",
    "DEVICE = 'cuda'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123bff6a-feb1-4278-8d2b-8d1254aeb03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                            0\n",
       "prompt                            What is physical mathematics?\n",
       "A                 The study of physically motivated mathematics\n",
       "B                             The study of mathematical physics\n",
       "C                 The study of mathematics in physical contexts\n",
       "D                           The study of mathematical equations\n",
       "E                          The study of mathematical operations\n",
       "answer                                                        A\n",
       "wiki_text     The subject of physical mathematics is concern...\n",
       "page_id                                                32439784\n",
       "page_title                                 Physical mathematics\n",
       "stem_label                                                    M\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = pd.read_csv('./data/crawl_context.csv')\n",
    "prompt.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75640d28-5bb8-4283-b56e-248c1195a8b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The subject of physical mathematics is concerned with physically motivated mathematics and is considered by some as a subfield of mathematical physics.\\nAccording to Margaret Osler the simple machines of Hero of Alexandria and the ray tracing of Alhazen did not refer to causality or forces. Accordingly these early expressions of kinematics and optics do not rise to the level of mathematical physics as practiced by Galileo and Newton.\\nThe details of physical units and their manipulation were addressed by Alexander Macfarlane in Physical Arithmetic in 1885. The science of kinematics created a need for mathematical representation of motion and has found expression with complex numbers, quaternions, and linear algebra.\\nAt Cambridge University the Mathematical Tripos tested students on their knowledge of \"mixed mathematics\". \"... [N]ew books which appeared  in the mid-eighteenth century offered a systematic introduction to the fundamental  operations of the fluxional calculus and showed how it could be applied to a wide range of mathematical and physical problems. ... The strongly problem-oriented presentation in the treatises ...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.loc[0,'wiki_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ceb482-91e8-46e9-9006-a671aa977253",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The study of physically motivated mathematics'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.loc[0,'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc02f49-f811-4518-952a-458ea5cbd9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49284, 12)\n"
     ]
    }
   ],
   "source": [
    "prompt = pd.read_csv('./data/crawl_context.csv')\n",
    "print(prompt.shape)\n",
    "prompt['prompt_answer'] = prompt.apply(lambda row: ' '.join(str(row[field]) for field in ['prompt', 'A', 'B', 'C', 'D', 'E']), axis=1)\n",
    "wiki = []\n",
    "with open('./data/wiki_data.json', 'r',encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        wiki.append(json.loads(line))\n",
    "wiki = pd.DataFrame(wiki)\n",
    "wiki['title_text'] = wiki.apply(lambda row : ' '.join(str(row[field]) for field in ['title','content']),axis=1)\n",
    "wiki = wiki[['page_id', 'title_text']]\n",
    "wiki.drop_duplicates(inplace=True)\n",
    "wiki = wiki.reset_index(drop=True)\n",
    "prompt.drop_duplicates(inplace=True)\n",
    "prompt = prompt.reset_index(drop=True)\n",
    "wiki['page_id'] = wiki['page_id'].apply(lambda x : int(x))\n",
    "prompt = pd.merge(prompt, wiki, on='page_id')\n",
    "prompt = prompt[['id','page_id','prompt_answer','wiki_text','title_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0af3851-77aa-44b5-be62-1fb4f0270a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Physical mathematics The subject of physical mathematics is concerned with physically motivated mathematics and is considered by some as a subfield of mathematical physics.\\nAccording to Margaret Osler the simple machines of Hero of Alexandria and the ray tracing of Alhazen did not refer to causality or forces. Accordingly these early expressions of kinematics and optics do not rise to the level of mathematical physics as practiced by Galileo and Newton.\\nThe details of physical units and their manipulation were addressed by Alexander Macfarlane in Physical Arithmetic in 1885. The science of kinematics created a need for mathematical representation of motion and has found expression with complex numbers, quaternions, and linear algebra.\\nAt Cambridge University the Mathematical Tripos tested students on their knowledge of \"mixed mathematics\". \"... [N]ew books which appeared  in the mid-eighteenth century offered a systematic introduction to the fundamental  operations of the fluxional calculus and showed how it could be applied to a wide range of mathematical and physical problems. ... The strongly problem-oriented presentation in the treatises ... made it much easier for university students to master the fluxional calculus and its applications [and] helped define a new field of mixed mathematical studies...\"\\nAn adventurous expression of physical mathematics is found in A Treatise on Electricity and Magnetism which used partial differential equations. The text aspired  to describe phenomena in four dimensions but the  foundation for this physical world, Minkowski space, trailed by forty years.\\nString theorist Greg Moore said this about physical mathematics in his vision talk at Strings 2014.\\n\"The use of the term “Physical Mathematics” in contrast to the more traditional “Mathematical Physics” by myself and others is not meant to detract from the venerable subject of Mathematical Physics but rather to delineate a smaller subfield characterized by questions and goals that are often motivated, on the physics side, by quantum gravity, string theory, and supersymmetry, (and more recently by the notion of topological phases in condensed matter physics), and, on the mathematics side, often involve deep relations to infinite-dimensional Lie algebras (and groups), topology, geometry, and even analytic number theory, in addition to the more traditional relations of physics to algebra, group theory, and analysis.\"\\n\\n\\n== See also ==\\nTheoretical physics\\nMathematical physics\\n\\n\\n== References ==\\n\\nEric Zaslow, Physmatics, arXiv:physics/0506153\\nArthur Jaffe, Frank Quinn, \"Theoretical mathematics: Toward a cultural synthesis of mathematics and theoretical physics\", Bulletin of the American Mathematical Society 30: 178-207, 1994, arXiv:math/9307227\\nMichael Atiyah et al., \"Responses to Theoretical Mathematics: Toward a cultural synthesis of mathematics and theoretical physics, by A. Jaffe and F. Quinn\", Bull. Am. Math. Soc. 30: 178-207, 1994, arXiv:math/9404229\\nMichael Stöltzner, \"Theoretical Mathematics: On the Philosophical Significance of the Jaffe-Quinn Debate\", in: The Role of Mathematics in Physical Sciences, pages 197-222, doi:10.1007/1-4020-3107-6_13\\nKevin Hartnett (November 30, 2017) \"Secret link discovered between pure math and physics\", Quanta Magazine'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.loc[0,'title_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5843260-3c66-45ed-ba78-a163d841bed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49284.000000\n",
       "mean       224.731089\n",
       "std        117.116009\n",
       "min         36.000000\n",
       "50%        202.000000\n",
       "98%        524.000000\n",
       "99%        600.000000\n",
       "max       2129.000000\n",
       "Name: prompt_answer, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt['prompt_answer'].apply(lambda x: len(x) if str(x)!='nan' else 0 ).describe([0.98,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e8c2558-8632-400e-bb00-3723085e0e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     49284.000000\n",
       "mean      10037.972770\n",
       "std       13972.651941\n",
       "min           5.000000\n",
       "50%        5239.000000\n",
       "70%        9664.300000\n",
       "98%       50666.000000\n",
       "99%       66664.000000\n",
       "max      229856.000000\n",
       "Name: title_text, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt['title_text'].apply(lambda x: len(x) if str(x)!='nan' else 0 ).describe([0.7,0.98,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0afdfc8c-d8b2-4006-a852-c54bd0e57f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMRecallDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BERT_PATH, use_fast=True)\n",
    "        self.query = []\n",
    "        self.answer = []\n",
    "        print('加载数据集中')\n",
    "        for i in tqdm(range(len(data))):\n",
    "            query = data.loc[i, 'prompt_answer']\n",
    "            answer = data.loc[i, 'title_text']\n",
    "            query_id = self.tokenizer.encode(query, add_special_tokens=False)\n",
    "            answer_id = self.tokenizer.encode(answer, add_special_tokens=False)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            if len(query_id) > 510:\n",
    "                query_id = [101] + query_id[:510] + [102]\n",
    "            else:\n",
    "                query_id = [101] + query_id + [102]\n",
    "            if len(answer_id) > 510:\n",
    "                answer_id = [101] + answer_id[:510] + [102]\n",
    "            else:\n",
    "                answer_id = [101] + answer_id + [102]\n",
    "            self.query.append(query_id)\n",
    "            self.answer.append(answer_id)\n",
    "    def __len__(self):\n",
    "        return len(self.query) \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.query[index], self.answer[index]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        def sequence_padding(inputs, length=None, padding=0):\n",
    "            \"\"\"\n",
    "            Numpy函数，将序列padding到同一长度\n",
    "            \"\"\"\n",
    "            if length is None:\n",
    "                length = max([len(x) for x in inputs])\n",
    "\n",
    "            pad_width = [(0, 0) for _ in np.shape(inputs[0])]\n",
    "            outputs = []\n",
    "            for x in inputs:\n",
    "                x = x[:length]\n",
    "                pad_width[0] = (0, length - len(x))\n",
    "                x = np.pad(x, pad_width, 'constant', constant_values=padding)\n",
    "                outputs.append(x)\n",
    "\n",
    "            return np.array(outputs, dtype='int64')\n",
    "        batch_query, batch_answer = [], []\n",
    "        \n",
    "        for item in batch:\n",
    "            query, answer = item\n",
    "            batch_query.append(query)\n",
    "            batch_answer.append(answer)\n",
    "        batch_query = torch.tensor(sequence_padding(batch_query), dtype=torch.long)\n",
    "        batch_answer = torch.tensor(sequence_padding(batch_answer), dtype=torch.long)\n",
    "        \n",
    "        return batch_query, batch_answer\n",
    "\n",
    "        \n",
    "class DataLoaderX(torch.utils.data.DataLoader):\n",
    "    '''\n",
    "        replace DataLoader with PrefetchDataLoader\n",
    "    '''\n",
    "    def __iter__(self):\n",
    "        return BackgroundGenerator(super().__iter__())  \n",
    "\n",
    "    \n",
    "def get_loader(prompt,batch_size,train_mode=True,num_workers=4):\n",
    "    ds_df = LLMRecallDataSet(prompt)\n",
    "    loader = DataLoaderX(ds_df, batch_size=batch_size if train_mode else batch_size//2, shuffle=train_mode, num_workers=num_workers,pin_memory=True,\n",
    "                                         collate_fn=ds_df.collate_fn, drop_last=train_mode)\n",
    "    loader.num = len(ds_df)\n",
    "    return loader\n",
    "\n",
    "def debug_loader(prompt, batch_size):\n",
    "    loader=get_loader(prompt,batch_size,train_mode=True,num_workers=0)\n",
    "    for token_ids,labels in loader:\n",
    "        print(token_ids)\n",
    "        print(labels)\n",
    "        break\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb15925-79dd-4a3a-a74f-65c291648bf0",
   "metadata": {},
   "source": [
    "# define recall model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffe727c-ff7d-4e92-ac50-58e09b451357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecallModel, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(BERT_PATH)\n",
    "    \n",
    "    def mask_mean(self, x, mask=None):\n",
    "        if mask != None:\n",
    "            mask_x = x * (mask.unsqueeze(-1))\n",
    "            x_sum = torch.sum(mask_x, dim=1)\n",
    "            re_x = torch.div(x_sum, torch.sum(mask, dim=1).unsqueeze(-1))\n",
    "        else:\n",
    "            x_sum = torch.sum(x, dim=1)\n",
    "            re_x = torch.div(x_sum, x.size()[1])\n",
    "        return re_x\n",
    "    \n",
    "    def forward(self,input_ids):\n",
    "        attention_mask = input_ids > 0\n",
    "        out = self.bert_model(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x = out[:,0,:]\n",
    "        return x\n",
    "\n",
    "def debug_label():\n",
    "    loader=get_loader(prompt,batch_size=2,train_mode=True,num_workers=0)\n",
    "    model= RecallModel()\n",
    "    print('models paramters:', sum(p.numel() for p in model.parameters()))\n",
    "    for token_ids,labels in loader:\n",
    "        # print(token_ids)\n",
    "        # print(labels)\n",
    "        prob=model(token_ids)\n",
    "        print(prob)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dcdaf29-eaee-47af-b0ee-60b4c810725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimCSE_loss(topic_pred,content_pred,tau=0.05):\n",
    "    similarities = F.cosine_similarity(topic_pred.unsqueeze(1), content_pred.unsqueeze(0), dim=2) # B,B\n",
    "    y_true = torch.arange(0,topic_pred.size(0)).to(DEVICE)\n",
    "    # similarities = similarities - torch.eye(pred.shape[0]) * 1e12\n",
    "    similarities = similarities / tau\n",
    "    loss=F.cross_entropy(similarities, y_true)\n",
    "    return torch.mean(loss)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "def trainer(train_dataloader,val_dataloader,model, epochs, model_save_path,\n",
    "            accumulation_steps=1, early_stop_epochs=5, device='cpu'):\n",
    "    ########早停\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    ########优化器 学习率\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    crf_p=[n for n, p in param_optimizer if str(n).find('crf')!=-1]\n",
    "    print(crf_p)\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and n not in crf_p], 'weight_decay': 0.8},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)  and n not in crf_p], 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in param_optimizer if n in crf_p], 'lr': 2e-3, 'weight_decay': 0.8},\n",
    "            ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_len = len(train_dataloader)\n",
    "\n",
    "    best_score = 100000000\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        bar = tqdm(train_dataloader)\n",
    "        for i, inputs in enumerate(bar):\n",
    "            with autocast():\n",
    "                topic_inputs,content_inputs = (_.to(device) for _ in inputs)\n",
    "                # print(topic_inputs.size())\n",
    "                # print(content_inputs.size())\n",
    "                topic_pred = model(topic_inputs)\n",
    "                content_pred = model(content_inputs)\n",
    "                # print(topic_pred.size())\n",
    "                # print(content_pred.size())\n",
    "                loss = SimCSE_loss(topic_pred,content_pred)\n",
    "            scaler.scale(loss).backward()\n",
    "            losses.append(loss.item())\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == train_len:\n",
    "                scaler.step(optimizer)\n",
    "                optimizer.zero_grad()\n",
    "                scaler.update()\n",
    "            bar.set_postfix(loss_mean=np.array(losses).mean(), epoch=epoch)\n",
    "        if epoch % 20 == 0:\n",
    "            torch.save(model.state_dict(), f'./save/recall/recall_epoch{epoch}.bin')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32acf498-50c4-41cc-8f61-5551839c8021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据集中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47001 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 47001/47001 [05:47<00:00, 135.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据集中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2284 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1052 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 2284/2284 [00:13<00:00, 166.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 367/367 [01:43<00:00,  3.54it/s, epoch=1, loss_mean=0.219]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=2, loss_mean=0.154]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.59it/s, epoch=3, loss_mean=0.125]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=4, loss_mean=0.107]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=5, loss_mean=0.0951]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=6, loss_mean=0.0859]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=7, loss_mean=0.0788]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=8, loss_mean=0.0731]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=9, loss_mean=0.0684]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=10, loss_mean=0.0644]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=11, loss_mean=0.0609]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=12, loss_mean=0.0579]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=13, loss_mean=0.0552]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=14, loss_mean=0.053] \n",
      "100%|██████████| 367/367 [01:42<00:00,  3.56it/s, epoch=15, loss_mean=0.051] \n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=16, loss_mean=0.0492]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=17, loss_mean=0.0475]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=18, loss_mean=0.046] \n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=19, loss_mean=0.0445]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=20, loss_mean=0.0433]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=21, loss_mean=0.0421]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=22, loss_mean=0.041] \n",
      "100%|██████████| 367/367 [01:42<00:00,  3.56it/s, epoch=23, loss_mean=0.04]  \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=24, loss_mean=0.0391]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.56it/s, epoch=25, loss_mean=0.0382]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.58it/s, epoch=26, loss_mean=0.0374]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=27, loss_mean=0.0366]\n",
      "100%|██████████| 367/367 [01:44<00:00,  3.52it/s, epoch=28, loss_mean=0.0359]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=29, loss_mean=0.0353]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=30, loss_mean=0.0346]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=31, loss_mean=0.034] \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=32, loss_mean=0.0334]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=33, loss_mean=0.0328]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=34, loss_mean=0.0323]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=35, loss_mean=0.0318]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=36, loss_mean=0.0314]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=37, loss_mean=0.0309]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.57it/s, epoch=38, loss_mean=0.0305]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=39, loss_mean=0.0301]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=40, loss_mean=0.0297]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=41, loss_mean=0.0293]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=42, loss_mean=0.029] \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=43, loss_mean=0.0287]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=44, loss_mean=0.0283]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=45, loss_mean=0.028] \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=46, loss_mean=0.0277]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=47, loss_mean=0.0274]\n",
      "100%|██████████| 367/367 [01:42<00:00,  3.56it/s, epoch=48, loss_mean=0.0271]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=49, loss_mean=0.0268]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=50, loss_mean=0.0266]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=51, loss_mean=0.0263]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=52, loss_mean=0.0261]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.54it/s, epoch=53, loss_mean=0.0258]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=54, loss_mean=0.0256]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=55, loss_mean=0.0254]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=56, loss_mean=0.0251]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=57, loss_mean=0.0249]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=58, loss_mean=0.0247]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=59, loss_mean=0.0246]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=60, loss_mean=0.0243]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=61, loss_mean=0.0242]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=62, loss_mean=0.024] \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=63, loss_mean=0.0238]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=64, loss_mean=0.0236]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=65, loss_mean=0.0235]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=66, loss_mean=0.0233]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=67, loss_mean=0.0232]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=68, loss_mean=0.023] \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=69, loss_mean=0.0228]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=70, loss_mean=0.0227]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=71, loss_mean=0.0226]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=72, loss_mean=0.0224]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=73, loss_mean=0.0223]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=74, loss_mean=0.0222]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=75, loss_mean=0.022] \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=76, loss_mean=0.0219]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.54it/s, epoch=77, loss_mean=0.0218]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=78, loss_mean=0.0217]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=79, loss_mean=0.0215]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=80, loss_mean=0.0214]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=81, loss_mean=0.0213]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=82, loss_mean=0.0212]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.54it/s, epoch=83, loss_mean=0.0211]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=84, loss_mean=0.021] \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=85, loss_mean=0.0209]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=86, loss_mean=0.0208]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=87, loss_mean=0.0207]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=88, loss_mean=0.0206]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=89, loss_mean=0.0205]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.54it/s, epoch=90, loss_mean=0.0204]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=91, loss_mean=0.0203]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=92, loss_mean=0.0202]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=93, loss_mean=0.0202]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.56it/s, epoch=94, loss_mean=0.0201]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=95, loss_mean=0.02]  \n",
      "100%|██████████| 367/367 [01:43<00:00,  3.55it/s, epoch=96, loss_mean=0.0199]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.54it/s, epoch=97, loss_mean=0.0198]\n",
      "100%|██████████| 367/367 [01:44<00:00,  3.52it/s, epoch=98, loss_mean=0.0198]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.53it/s, epoch=99, loss_mean=0.0197]\n",
      "100%|██████████| 367/367 [01:43<00:00,  3.53it/s, epoch=100, loss_mean=0.0196]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9519906044006348,\n",
       " 1.8216842412948608,\n",
       " 1.792602777481079,\n",
       " 1.6925252676010132,\n",
       " 1.3921329975128174,\n",
       " 1.309242606163025,\n",
       " 1.0707662105560303,\n",
       " 0.9850286245346069,\n",
       " 0.7344101667404175,\n",
       " 0.7770193815231323,\n",
       " 0.7651268243789673,\n",
       " 0.7516265511512756,\n",
       " 0.5482092499732971,\n",
       " 0.5104630589485168,\n",
       " 0.5680720210075378,\n",
       " 0.5906234383583069,\n",
       " 0.6495450139045715,\n",
       " 0.46312934160232544,\n",
       " 0.529392421245575,\n",
       " 0.39064404368400574,\n",
       " 0.3021797239780426,\n",
       " 0.3385663330554962,\n",
       " 0.34318941831588745,\n",
       " 0.38456010818481445,\n",
       " 0.4530014395713806,\n",
       " 0.4653345048427582,\n",
       " 0.4079946279525757,\n",
       " 0.37095069885253906,\n",
       " 0.3217395842075348,\n",
       " 0.2944546639919281,\n",
       " 0.43370068073272705,\n",
       " 0.3861745297908783,\n",
       " 0.4166041910648346,\n",
       " 0.37937766313552856,\n",
       " 0.2808482348918915,\n",
       " 0.33334943652153015,\n",
       " 0.4106549024581909,\n",
       " 0.36930418014526367,\n",
       " 0.23981483280658722,\n",
       " 0.24083928763866425,\n",
       " 0.5208274126052856,\n",
       " 0.41099750995635986,\n",
       " 0.33048388361930847,\n",
       " 0.3470776677131653,\n",
       " 0.4313044846057892,\n",
       " 0.32513928413391113,\n",
       " 0.30208033323287964,\n",
       " 0.2565422058105469,\n",
       " 0.3415522575378418,\n",
       " 0.25031107664108276,\n",
       " 0.16105028986930847,\n",
       " 0.2845456004142761,\n",
       " 0.28326064348220825,\n",
       " 0.20201051235198975,\n",
       " 0.34652477502822876,\n",
       " 0.26911020278930664,\n",
       " 0.25596266984939575,\n",
       " 0.27005621790885925,\n",
       " 0.273031085729599,\n",
       " 0.2845737934112549,\n",
       " 0.2930125594139099,\n",
       " 0.23123803734779358,\n",
       " 0.363811194896698,\n",
       " 0.24992768466472626,\n",
       " 0.22586068511009216,\n",
       " 0.25564971566200256,\n",
       " 0.1716773360967636,\n",
       " 0.28672918677330017,\n",
       " 0.19164490699768066,\n",
       " 0.23240035772323608,\n",
       " 0.1782599687576294,\n",
       " 0.16878950595855713,\n",
       " 0.25188976526260376,\n",
       " 0.3139692544937134,\n",
       " 0.2479829341173172,\n",
       " 0.2615509331226349,\n",
       " 0.276263564825058,\n",
       " 0.2262149602174759,\n",
       " 0.2939399480819702,\n",
       " 0.21960654854774475,\n",
       " 0.4057920575141907,\n",
       " 0.21255186200141907,\n",
       " 0.12981468439102173,\n",
       " 0.2339419424533844,\n",
       " 0.24629822373390198,\n",
       " 0.31691715121269226,\n",
       " 0.2970625162124634,\n",
       " 0.2955916225910187,\n",
       " 0.23445048928260803,\n",
       " 0.16725260019302368,\n",
       " 0.17664457857608795,\n",
       " 0.178067147731781,\n",
       " 0.29830479621887207,\n",
       " 0.17477735877037048,\n",
       " 0.17763099074363708,\n",
       " 0.12826856970787048,\n",
       " 0.1810140162706375,\n",
       " 0.26175615191459656,\n",
       " 0.20619481801986694,\n",
       " 0.12383371591567993,\n",
       " 0.22311930358409882,\n",
       " 0.14570483565330505,\n",
       " 0.2740899622440338,\n",
       " 0.14409145712852478,\n",
       " 0.16236545145511627,\n",
       " 0.16260714828968048,\n",
       " 0.24142567813396454,\n",
       " 0.16204911470413208,\n",
       " 0.12982434034347534,\n",
       " 0.2695673108100891,\n",
       " 0.27686795592308044,\n",
       " 0.14477454125881195,\n",
       " 0.16586516797542572,\n",
       " 0.15049198269844055,\n",
       " 0.28020328283309937,\n",
       " 0.18623775243759155,\n",
       " 0.11642845720052719,\n",
       " 0.16428543627262115,\n",
       " 0.1343090981245041,\n",
       " 0.11493879556655884,\n",
       " 0.19606608152389526,\n",
       " 0.16523082554340363,\n",
       " 0.16802826523780823,\n",
       " 0.20207832753658295,\n",
       " 0.25774598121643066,\n",
       " 0.19585178792476654,\n",
       " 0.2676610052585602,\n",
       " 0.11935638636350632,\n",
       " 0.1565096229314804,\n",
       " 0.13174863159656525,\n",
       " 0.15944014489650726,\n",
       " 0.20272722840309143,\n",
       " 0.14740735292434692,\n",
       " 0.13907840847969055,\n",
       " 0.15752804279327393,\n",
       " 0.1777082085609436,\n",
       " 0.2408488690853119,\n",
       " 0.16051137447357178,\n",
       " 0.17375262081623077,\n",
       " 0.22620505094528198,\n",
       " 0.17106406390666962,\n",
       " 0.13267570734024048,\n",
       " 0.16520808637142181,\n",
       " 0.16017213463783264,\n",
       " 0.14426954090595245,\n",
       " 0.07604572921991348,\n",
       " 0.1650768518447876,\n",
       " 0.18211089074611664,\n",
       " 0.18094420433044434,\n",
       " 0.282671183347702,\n",
       " 0.15154550969600677,\n",
       " 0.19863764941692352,\n",
       " 0.10594253242015839,\n",
       " 0.1702115684747696,\n",
       " 0.05623943731188774,\n",
       " 0.23039796948432922,\n",
       " 0.15791812539100647,\n",
       " 0.08970711380243301,\n",
       " 0.15971674025058746,\n",
       " 0.2329394817352295,\n",
       " 0.14898765087127686,\n",
       " 0.16228194534778595,\n",
       " 0.10896732658147812,\n",
       " 0.3018656373023987,\n",
       " 0.21342407166957855,\n",
       " 0.16015779972076416,\n",
       " 0.16574135422706604,\n",
       " 0.12021267414093018,\n",
       " 0.24252840876579285,\n",
       " 0.12295985966920853,\n",
       " 0.21122956275939941,\n",
       " 0.08134733140468597,\n",
       " 0.09792120009660721,\n",
       " 0.21367768943309784,\n",
       " 0.1922372579574585,\n",
       " 0.12402871996164322,\n",
       " 0.11089066416025162,\n",
       " 0.2475862354040146,\n",
       " 0.14161938428878784,\n",
       " 0.1370319128036499,\n",
       " 0.2126285880804062,\n",
       " 0.09006404131650925,\n",
       " 0.2954005002975464,\n",
       " 0.1814839392900467,\n",
       " 0.2298395335674286,\n",
       " 0.07897479832172394,\n",
       " 0.09178664535284042,\n",
       " 0.258267879486084,\n",
       " 0.14776255190372467,\n",
       " 0.17103290557861328,\n",
       " 0.20214597880840302,\n",
       " 0.17060576379299164,\n",
       " 0.15163487195968628,\n",
       " 0.1572779417037964,\n",
       " 0.22004440426826477,\n",
       " 0.17849190533161163,\n",
       " 0.0860341265797615,\n",
       " 0.09377211332321167,\n",
       " 0.15434856712818146,\n",
       " 0.1367207169532776,\n",
       " 0.10342732816934586,\n",
       " 0.1456749439239502,\n",
       " 0.1303885579109192,\n",
       " 0.13496291637420654,\n",
       " 0.1384197473526001,\n",
       " 0.10750244557857513,\n",
       " 0.17576517164707184,\n",
       " 0.18291763961315155,\n",
       " 0.09084741771221161,\n",
       " 0.1017904207110405,\n",
       " 0.2076674997806549,\n",
       " 0.05227909982204437,\n",
       " 0.15725849568843842,\n",
       " 0.15787220001220703,\n",
       " 0.05865917727351189,\n",
       " 0.15657015144824982,\n",
       " 0.13452057540416718,\n",
       " 0.10463947057723999,\n",
       " 0.27193883061408997,\n",
       " 0.126736119389534,\n",
       " 0.10148017108440399,\n",
       " 0.14624571800231934,\n",
       " 0.14245672523975372,\n",
       " 0.07049581408500671,\n",
       " 0.1427219659090042,\n",
       " 0.09874199330806732,\n",
       " 0.18430492281913757,\n",
       " 0.12031218409538269,\n",
       " 0.14782153069972992,\n",
       " 0.10554208606481552,\n",
       " 0.10645516216754913,\n",
       " 0.14294461905956268,\n",
       " 0.17326456308364868,\n",
       " 0.10399055480957031,\n",
       " 0.10730963200330734,\n",
       " 0.23324500024318695,\n",
       " 0.14734873175621033,\n",
       " 0.1758134961128235,\n",
       " 0.14276930689811707,\n",
       " 0.13152039051055908,\n",
       " 0.13092316687107086,\n",
       " 0.13045495748519897,\n",
       " 0.10388032346963882,\n",
       " 0.10480089485645294,\n",
       " 0.1597810983657837,\n",
       " 0.18977715075016022,\n",
       " 0.18522533774375916,\n",
       " 0.12203791737556458,\n",
       " 0.09243713319301605,\n",
       " 0.1408376693725586,\n",
       " 0.0837034285068512,\n",
       " 0.1114145964384079,\n",
       " 0.14528511464595795,\n",
       " 0.11047693341970444,\n",
       " 0.15387146174907684,\n",
       " 0.13065142929553986,\n",
       " 0.1079292744398117,\n",
       " 0.11915179342031479,\n",
       " 0.10046302527189255,\n",
       " 0.1343669593334198,\n",
       " 0.10827400535345078,\n",
       " 0.1271325945854187,\n",
       " 0.1324239820241928,\n",
       " 0.20682010054588318,\n",
       " 0.19073404371738434,\n",
       " 0.1358584314584732,\n",
       " 0.09334798902273178,\n",
       " 0.12579332292079926,\n",
       " 0.04472147300839424,\n",
       " 0.1751641184091568,\n",
       " 0.19745305180549622,\n",
       " 0.1577356457710266,\n",
       " 0.07355835288763046,\n",
       " 0.15018051862716675,\n",
       " 0.10154989361763,\n",
       " 0.08401234447956085,\n",
       " 0.1476314216852188,\n",
       " 0.11128030717372894,\n",
       " 0.14079169929027557,\n",
       " 0.1664031594991684,\n",
       " 0.10205857455730438,\n",
       " 0.15377876162528992,\n",
       " 0.11456219106912613,\n",
       " 0.19505929946899414,\n",
       " 0.13051600754261017,\n",
       " 0.0963936373591423,\n",
       " 0.10154271125793457,\n",
       " 0.13765078783035278,\n",
       " 0.09182677417993546,\n",
       " 0.10302961617708206,\n",
       " 0.21313033998012543,\n",
       " 0.14201000332832336,\n",
       " 0.16834932565689087,\n",
       " 0.16579963266849518,\n",
       " 0.08502741903066635,\n",
       " 0.15157020092010498,\n",
       " 0.1534513384103775,\n",
       " 0.11289839446544647,\n",
       " 0.23054270446300507,\n",
       " 0.15812699496746063,\n",
       " 0.11966293305158615,\n",
       " 0.09672318398952484,\n",
       " 0.06145324930548668,\n",
       " 0.14529548585414886,\n",
       " 0.19447825849056244,\n",
       " 0.11522367596626282,\n",
       " 0.17511838674545288,\n",
       " 0.10644266754388809,\n",
       " 0.0670986920595169,\n",
       " 0.12540511786937714,\n",
       " 0.138105109333992,\n",
       " 0.14633063971996307,\n",
       " 0.10611328482627869,\n",
       " 0.1385660171508789,\n",
       " 0.14632107317447662,\n",
       " 0.23430143296718597,\n",
       " 0.18787936866283417,\n",
       " 0.08860448747873306,\n",
       " 0.12291955202817917,\n",
       " 0.08391635119915009,\n",
       " 0.11384407430887222,\n",
       " 0.14015738666057587,\n",
       " 0.10436495393514633,\n",
       " 0.10650187730789185,\n",
       " 0.15864616632461548,\n",
       " 0.07301872968673706,\n",
       " 0.15000751614570618,\n",
       " 0.1309678554534912,\n",
       " 0.1268593817949295,\n",
       " 0.15564803779125214,\n",
       " 0.10899543762207031,\n",
       " 0.1675167977809906,\n",
       " 0.07217853516340256,\n",
       " 0.10162515938282013,\n",
       " 0.09064087271690369,\n",
       " 0.10732666403055191,\n",
       " 0.13942517340183258,\n",
       " 0.14892348647117615,\n",
       " 0.09348560124635696,\n",
       " 0.05670895799994469,\n",
       " 0.15714944899082184,\n",
       " 0.09178593009710312,\n",
       " 0.10117360204458237,\n",
       " 0.15162891149520874,\n",
       " 0.11239232122898102,\n",
       " 0.13501553237438202,\n",
       " 0.1981201171875,\n",
       " 0.11282084882259369,\n",
       " 0.051907364279031754,\n",
       " 0.09412325173616409,\n",
       " 0.09038103371858597,\n",
       " 0.071141317486763,\n",
       " 0.050060175359249115,\n",
       " 0.0948185920715332,\n",
       " 0.10235842317342758,\n",
       " 0.14282160997390747,\n",
       " 0.10698577761650085,\n",
       " 0.11287376284599304,\n",
       " 0.11547084897756577,\n",
       " 0.10395874083042145,\n",
       " 0.043106552213430405,\n",
       " 0.13695509731769562,\n",
       " 0.12277357280254364,\n",
       " 0.09193092584609985,\n",
       " 0.10183705389499664,\n",
       " 0.10372695326805115,\n",
       " 0.06371843814849854,\n",
       " 0.0680188238620758,\n",
       " 0.05137689411640167,\n",
       " 0.12052532285451889,\n",
       " 0.17606963217258453,\n",
       " 0.14418968558311462,\n",
       " 0.09978853166103363,\n",
       " 0.1231042891740799,\n",
       " 0.07993002980947495,\n",
       " 0.06454029679298401,\n",
       " 0.1386946439743042,\n",
       " 0.10367397218942642,\n",
       " 0.14637701213359833,\n",
       " 0.09454357624053955,\n",
       " 0.06470497697591782,\n",
       " 0.14678747951984406,\n",
       " 0.061134736984968185,\n",
       " 0.12206322699785233,\n",
       " 0.07346974313259125,\n",
       " 0.1256055384874344,\n",
       " 0.08433648943901062,\n",
       " 0.13825862109661102,\n",
       " 0.04310641810297966,\n",
       " 0.08588314801454544,\n",
       " 0.07483955472707748,\n",
       " 0.04269872233271599,\n",
       " 0.09334982186555862,\n",
       " 0.1199464425444603,\n",
       " 0.074526846408844,\n",
       " 0.062343496829271317,\n",
       " 0.09816001355648041,\n",
       " 0.06040545180439949,\n",
       " 0.04314196854829788,\n",
       " 0.11969571560621262,\n",
       " 0.0704406276345253,\n",
       " 0.15759897232055664,\n",
       " 0.10025855153799057,\n",
       " 0.12057368457317352,\n",
       " 0.1267409771680832,\n",
       " 0.1402355283498764,\n",
       " 0.04276436194777489,\n",
       " 0.0956970602273941,\n",
       " 0.06973133981227875,\n",
       " 0.10721682757139206,\n",
       " 0.05988505855202675,\n",
       " 0.10434595495462418,\n",
       " 0.09900351613759995,\n",
       " 0.15607032179832458,\n",
       " 0.14815211296081543,\n",
       " 0.13095369935035706,\n",
       " 0.09755617380142212,\n",
       " 0.11872463673353195,\n",
       " 0.06342630088329315,\n",
       " 0.054238464683294296,\n",
       " 0.056372642517089844,\n",
       " 0.14199429750442505,\n",
       " 0.07622063905000687,\n",
       " 0.09167364984750748,\n",
       " 0.10669263452291489,\n",
       " 0.09089290350675583,\n",
       " 0.0734718069434166,\n",
       " 0.049668289721012115,\n",
       " 0.14587141573429108,\n",
       " 0.06850005686283112,\n",
       " 0.08043331652879715,\n",
       " 0.07718484848737717,\n",
       " 0.05029470473527908,\n",
       " 0.17253750562667847,\n",
       " 0.08149539679288864,\n",
       " 0.09729118645191193,\n",
       " 0.08815625309944153,\n",
       " 0.06827620416879654,\n",
       " 0.20167240500450134,\n",
       " 0.07157299667596817,\n",
       " 0.076695017516613,\n",
       " 0.08540577441453934,\n",
       " 0.06494706124067307,\n",
       " 0.045534905046224594,\n",
       " 0.08920805901288986,\n",
       " 0.10489372164011002,\n",
       " 0.04895560070872307,\n",
       " 0.03820127248764038,\n",
       " 0.10401298850774765,\n",
       " 0.05643486976623535,\n",
       " 0.08832904696464539,\n",
       " 0.10884758085012436,\n",
       " 0.133376806974411,\n",
       " 0.07919029146432877,\n",
       " 0.06077771633863449,\n",
       " 0.07592162489891052,\n",
       " 0.08502718806266785,\n",
       " 0.058696381747722626,\n",
       " 0.09601523727178574,\n",
       " 0.09438732266426086,\n",
       " 0.10295224189758301,\n",
       " 0.13049514591693878,\n",
       " 0.13489800691604614,\n",
       " 0.07454533874988556,\n",
       " 0.08003246039152145,\n",
       " 0.1173945963382721,\n",
       " 0.08723389357328415,\n",
       " 0.06879809498786926,\n",
       " 0.07167117297649384,\n",
       " 0.11250092834234238,\n",
       " 0.07034200429916382,\n",
       " 0.08061328530311584,\n",
       " 0.08481770753860474,\n",
       " 0.17852075397968292,\n",
       " 0.07945337891578674,\n",
       " 0.10874755680561066,\n",
       " 0.08762793242931366,\n",
       " 0.1369650810956955,\n",
       " 0.10370989888906479,\n",
       " 0.08125912398099899,\n",
       " 0.13092561066150665,\n",
       " 0.10634556412696838,\n",
       " 0.05905425176024437,\n",
       " 0.10151955485343933,\n",
       " 0.045171402394771576,\n",
       " 0.12075860798358917,\n",
       " 0.09695404767990112,\n",
       " 0.09624500572681427,\n",
       " 0.12609189748764038,\n",
       " 0.0738748088479042,\n",
       " 0.0888107493519783,\n",
       " 0.08602795004844666,\n",
       " 0.09752199053764343,\n",
       " 0.08355098962783813,\n",
       " 0.06401819735765457,\n",
       " 0.13299746811389923,\n",
       " 0.1010817140340805,\n",
       " 0.1103033795952797,\n",
       " 0.040098655968904495,\n",
       " 0.06649701297283173,\n",
       " 0.09702783077955246,\n",
       " 0.07124686986207962,\n",
       " 0.09967930614948273,\n",
       " 0.16765296459197998,\n",
       " 0.11970163136720657,\n",
       " 0.09605731070041656,\n",
       " 0.062066901475191116,\n",
       " 0.15692982077598572,\n",
       " 0.05434930697083473,\n",
       " 0.10661500692367554,\n",
       " 0.056658048182725906,\n",
       " 0.07780463993549347,\n",
       " 0.08578304946422577,\n",
       " 0.09068439155817032,\n",
       " 0.06450893729925156,\n",
       " 0.1090766116976738,\n",
       " 0.07377468794584274,\n",
       " 0.13380120694637299,\n",
       " 0.05958537012338638,\n",
       " 0.09850961714982986,\n",
       " 0.08286682516336441,\n",
       " 0.05328313261270523,\n",
       " 0.10719844698905945,\n",
       " 0.06888144463300705,\n",
       " 0.1094585731625557,\n",
       " 0.04916546866297722,\n",
       " 0.09630884230136871,\n",
       " 0.04168201610445976,\n",
       " 0.0702873170375824,\n",
       " 0.0694107636809349,\n",
       " 0.0530245266854763,\n",
       " 0.08749063313007355,\n",
       " 0.10426429659128189,\n",
       " 0.07109828293323517,\n",
       " 0.07015284150838852,\n",
       " 0.08917588740587234,\n",
       " 0.050096187740564346,\n",
       " 0.08857885003089905,\n",
       " 0.09640852361917496,\n",
       " 0.09540890902280807,\n",
       " 0.1022963672876358,\n",
       " 0.060120921581983566,\n",
       " 0.12111065536737442,\n",
       " 0.13903070986270905,\n",
       " 0.056722089648246765,\n",
       " 0.04523452743887901,\n",
       " 0.10766732692718506,\n",
       " 0.08805329352617264,\n",
       " 0.09891872853040695,\n",
       " 0.1433996856212616,\n",
       " 0.0917070135474205,\n",
       " 0.046076200902462006,\n",
       " 0.06196562200784683,\n",
       " 0.05405468866229057,\n",
       " 0.15477964282035828,\n",
       " 0.12504106760025024,\n",
       " 0.0628151148557663,\n",
       " 0.1001182347536087,\n",
       " 0.07818164676427841,\n",
       " 0.05435732752084732,\n",
       " 0.06964553147554398,\n",
       " 0.08918704092502594,\n",
       " 0.11825001239776611,\n",
       " 0.08802689611911774,\n",
       " 0.13551932573318481,\n",
       " 0.0637064203619957,\n",
       " 0.10992156714200974,\n",
       " 0.05114961415529251,\n",
       " 0.12807555496692657,\n",
       " 0.051678616553545,\n",
       " 0.11283637583255768,\n",
       " 0.08420977741479874,\n",
       " 0.07043356448411942,\n",
       " 0.09714842587709427,\n",
       " 0.1827545166015625,\n",
       " 0.05643000081181526,\n",
       " 0.08875612169504166,\n",
       " 0.10374792665243149,\n",
       " 0.10217496752738953,\n",
       " 0.08545538783073425,\n",
       " 0.10686127841472626,\n",
       " 0.16512006521224976,\n",
       " 0.13251881301403046,\n",
       " 0.057639069855213165,\n",
       " 0.09420479834079742,\n",
       " 0.048439525067806244,\n",
       " 0.05787479877471924,\n",
       " 0.10322940349578857,\n",
       " 0.09270823001861572,\n",
       " 0.06325095146894455,\n",
       " 0.09608617424964905,\n",
       " 0.09269172698259354,\n",
       " 0.13689051568508148,\n",
       " 0.03127841651439667,\n",
       " 0.057252585887908936,\n",
       " 0.0981568843126297,\n",
       " 0.07705391198396683,\n",
       " 0.068264901638031,\n",
       " 0.08921832591295242,\n",
       " 0.14006876945495605,\n",
       " 0.07057425379753113,\n",
       " 0.09612486511468887,\n",
       " 0.10691027343273163,\n",
       " 0.07693689316511154,\n",
       " 0.05178603529930115,\n",
       " 0.08133294433355331,\n",
       " 0.081245057284832,\n",
       " 0.1308271884918213,\n",
       " 0.11763983964920044,\n",
       " 0.13184717297554016,\n",
       " 0.07589872181415558,\n",
       " 0.05045035853981972,\n",
       " 0.12428270280361176,\n",
       " 0.07527647167444229,\n",
       " 0.1330096274614334,\n",
       " 0.1302442103624344,\n",
       " 0.041283197700977325,\n",
       " 0.09129316359758377,\n",
       " 0.06213109567761421,\n",
       " 0.08250883966684341,\n",
       " 0.060319073498249054,\n",
       " 0.1407313495874405,\n",
       " 0.0649198368191719,\n",
       " 0.060653217136859894,\n",
       " 0.06700827181339264,\n",
       " 0.11320755630731583,\n",
       " 0.05775712430477142,\n",
       " 0.06326062977313995,\n",
       " 0.11891671270132065,\n",
       " 0.07030346989631653,\n",
       " 0.12878981232643127,\n",
       " 0.102837473154068,\n",
       " 0.1433880776166916,\n",
       " 0.06412868201732635,\n",
       " 0.0942293182015419,\n",
       " 0.08370530605316162,\n",
       " 0.1089504063129425,\n",
       " 0.08640459924936295,\n",
       " 0.07995910942554474,\n",
       " 0.09709981828927994,\n",
       " 0.0550939179956913,\n",
       " 0.09231586754322052,\n",
       " 0.10681744664907455,\n",
       " 0.10935106128454208,\n",
       " 0.08863336592912674,\n",
       " 0.09619918465614319,\n",
       " 0.05732821300625801,\n",
       " 0.04757241904735565,\n",
       " 0.050627924501895905,\n",
       " 0.07636576890945435,\n",
       " 0.1768539547920227,\n",
       " 0.0797119066119194,\n",
       " 0.055524792522192,\n",
       " 0.08957884460687637,\n",
       " 0.07252699136734009,\n",
       " 0.10140049457550049,\n",
       " 0.04982078820466995,\n",
       " 0.08875773102045059,\n",
       " 0.030415387824177742,\n",
       " 0.10838422924280167,\n",
       " 0.10990844666957855,\n",
       " 0.07235656678676605,\n",
       " 0.036228254437446594,\n",
       " 0.03810533881187439,\n",
       " 0.05173927918076515,\n",
       " 0.09857416898012161,\n",
       " 0.08448748290538788,\n",
       " 0.12069511413574219,\n",
       " 0.036983031779527664,\n",
       " 0.09055603295564651,\n",
       " 0.09790486097335815,\n",
       " 0.1074318215250969,\n",
       " 0.07400579005479813,\n",
       " 0.056786149740219116,\n",
       " 0.14070698618888855,\n",
       " 0.1510746330022812,\n",
       " 0.06686089932918549,\n",
       " 0.12821465730667114,\n",
       " 0.09227754920721054,\n",
       " 0.13853920996189117,\n",
       " 0.09889733791351318,\n",
       " 0.07168416678905487,\n",
       " 0.060340866446495056,\n",
       " 0.13402311503887177,\n",
       " 0.0641031265258789,\n",
       " 0.09354859590530396,\n",
       " 0.09936309605836868,\n",
       " 0.10398344695568085,\n",
       " 0.05923193693161011,\n",
       " 0.10829313844442368,\n",
       " 0.06985316425561905,\n",
       " 0.08365198224782944,\n",
       " 0.10027806460857391,\n",
       " 0.07315626740455627,\n",
       " 0.060893040150403976,\n",
       " 0.06627896428108215,\n",
       " 0.1215151771903038,\n",
       " 0.06210240721702576,\n",
       " 0.08947499096393585,\n",
       " 0.11493491381406784,\n",
       " 0.07529096305370331,\n",
       " 0.09383982419967651,\n",
       " 0.0795006975531578,\n",
       " 0.06636006385087967,\n",
       " 0.05825292319059372,\n",
       " 0.07031688094139099,\n",
       " 0.04252347722649574,\n",
       " 0.05131562799215317,\n",
       " 0.08690256625413895,\n",
       " 0.05493246763944626,\n",
       " 0.09266000986099243,\n",
       " 0.06838393956422806,\n",
       " 0.12185249477624893,\n",
       " 0.06667480617761612,\n",
       " 0.0753726065158844,\n",
       " 0.07203899323940277,\n",
       " 0.10476166009902954,\n",
       " 0.07848257571458817,\n",
       " 0.10052763670682907,\n",
       " 0.037761662155389786,\n",
       " 0.08632680773735046,\n",
       " 0.07063300907611847,\n",
       " 0.10196346789598465,\n",
       " 0.0384855642914772,\n",
       " 0.04103267937898636,\n",
       " 0.06720729172229767,\n",
       " 0.10091204196214676,\n",
       " 0.029497923329472542,\n",
       " 0.07411632686853409,\n",
       " 0.05649138242006302,\n",
       " 0.049393896013498306,\n",
       " 0.13510099053382874,\n",
       " 0.06664411723613739,\n",
       " 0.12733541429042816,\n",
       " 0.06940124928951263,\n",
       " 0.0710865780711174,\n",
       " 0.09545295685529709,\n",
       " 0.045702118426561356,\n",
       " 0.07272623479366302,\n",
       " 0.04359864071011543,\n",
       " 0.07680698484182358,\n",
       " 0.0536285899579525,\n",
       " 0.020721998065710068,\n",
       " 0.13168461620807648,\n",
       " 0.06628233194351196,\n",
       " 0.05754667520523071,\n",
       " 0.04578481987118721,\n",
       " 0.05271004140377045,\n",
       " 0.10390676558017731,\n",
       " 0.07962070405483246,\n",
       " 0.04040786623954773,\n",
       " 0.13255375623703003,\n",
       " 0.05711240693926811,\n",
       " 0.05697711557149887,\n",
       " 0.10167412459850311,\n",
       " 0.07312414795160294,\n",
       " 0.028478018939495087,\n",
       " 0.08945673704147339,\n",
       " 0.05055806040763855,\n",
       " 0.10092136263847351,\n",
       " 0.071933314204216,\n",
       " 0.08354450762271881,\n",
       " 0.03642489016056061,\n",
       " 0.014251938089728355,\n",
       " 0.07207954674959183,\n",
       " 0.05377402529120445,\n",
       " 0.09403011947870255,\n",
       " 0.03730121627449989,\n",
       " 0.08261848241090775,\n",
       " 0.07031339406967163,\n",
       " 0.08171842247247696,\n",
       " 0.05625326931476593,\n",
       " 0.061495546251535416,\n",
       " 0.09324581176042557,\n",
       " 0.11969111859798431,\n",
       " 0.032258763909339905,\n",
       " 0.07485612481832504,\n",
       " 0.07474151998758316,\n",
       " 0.03375851735472679,\n",
       " 0.10415708273649216,\n",
       " 0.06295795738697052,\n",
       " 0.03586016222834587,\n",
       " 0.06186039745807648,\n",
       " 0.03042125329375267,\n",
       " 0.07244198769330978,\n",
       " 0.03407720848917961,\n",
       " 0.06116196885704994,\n",
       " 0.058778710663318634,\n",
       " 0.043716251850128174,\n",
       " 0.1025741770863533,\n",
       " 0.09930934756994247,\n",
       " 0.06293029338121414,\n",
       " 0.05696681886911392,\n",
       " 0.09107673913240433,\n",
       " 0.07210071384906769,\n",
       " 0.09021008014678955,\n",
       " 0.0766715481877327,\n",
       " 0.07612809538841248,\n",
       " 0.05033256486058235,\n",
       " 0.07214971631765366,\n",
       " 0.10026447474956512,\n",
       " 0.08706464618444443,\n",
       " 0.06311880052089691,\n",
       " 0.13404715061187744,\n",
       " 0.0640416145324707,\n",
       " 0.05713794752955437,\n",
       " 0.07413449883460999,\n",
       " 0.05729427561163902,\n",
       " 0.04401311278343201,\n",
       " 0.05593714118003845,\n",
       " 0.0910588800907135,\n",
       " 0.06511974334716797,\n",
       " 0.11855890601873398,\n",
       " 0.04084270820021629,\n",
       " 0.061004795134067535,\n",
       " 0.09232240915298462,\n",
       " 0.048106297850608826,\n",
       " 0.0751190334558487,\n",
       " 0.05288077890872955,\n",
       " 0.08374671638011932,\n",
       " 0.08734993636608124,\n",
       " 0.05619603022933006,\n",
       " 0.05983072891831398,\n",
       " 0.030893515795469284,\n",
       " 0.05527573078870773,\n",
       " 0.057040534913539886,\n",
       " 0.09261289238929749,\n",
       " 0.08461124449968338,\n",
       " 0.04881671443581581,\n",
       " 0.09904064983129501,\n",
       " 0.10316438972949982,\n",
       " 0.09853953123092651,\n",
       " 0.07413970679044724,\n",
       " 0.04462548717856407,\n",
       " 0.06746344268321991,\n",
       " 0.10816872864961624,\n",
       " 0.08613170683383942,\n",
       " 0.03569545969367027,\n",
       " 0.02767922542989254,\n",
       " 0.04904822260141373,\n",
       " 0.0370565727353096,\n",
       " 0.07078912854194641,\n",
       " 0.06676304340362549,\n",
       " 0.04015684127807617,\n",
       " 0.10591466724872589,\n",
       " 0.054815858602523804,\n",
       " 0.05721841752529144,\n",
       " 0.06395391374826431,\n",
       " 0.07902141660451889,\n",
       " 0.07294962555170059,\n",
       " 0.08266965299844742,\n",
       " 0.059124305844306946,\n",
       " 0.03515435382723808,\n",
       " 0.0613851435482502,\n",
       " 0.03608207404613495,\n",
       " 0.04375500604510307,\n",
       " 0.07094019651412964,\n",
       " 0.06974998861551285,\n",
       " 0.09014749526977539,\n",
       " 0.05227432772517204,\n",
       " 0.033362895250320435,\n",
       " 0.065660759806633,\n",
       " 0.03117593564093113,\n",
       " 0.08656365424394608,\n",
       " 0.07622046023607254,\n",
       " 0.08405343443155289,\n",
       " 0.029916133731603622,\n",
       " 0.05507201701402664,\n",
       " 0.09612397104501724,\n",
       " 0.08753985911607742,\n",
       " 0.08592262864112854,\n",
       " 0.07934005558490753,\n",
       " 0.044083673506975174,\n",
       " 0.04834309220314026,\n",
       " 0.053107623010873795,\n",
       " 0.06384340673685074,\n",
       " 0.11011551320552826,\n",
       " 0.06190634146332741,\n",
       " 0.026052406057715416,\n",
       " 0.04651328921318054,\n",
       " 0.06135030835866928,\n",
       " 0.039889492094516754,\n",
       " 0.07563941925764084,\n",
       " 0.0537831149995327,\n",
       " 0.0678795725107193,\n",
       " 0.023170489817857742,\n",
       " 0.08884448558092117,\n",
       " 0.06133522465825081,\n",
       " 0.0642285943031311,\n",
       " 0.05980703979730606,\n",
       " 0.05606943368911743,\n",
       " 0.07980307191610336,\n",
       " 0.08593684434890747,\n",
       " 0.060945022851228714,\n",
       " 0.08748069405555725,\n",
       " 0.07372447848320007,\n",
       " 0.06225679814815521,\n",
       " 0.1589188277721405,\n",
       " 0.05318375304341316,\n",
       " 0.0712398812174797,\n",
       " 0.08129365742206573,\n",
       " 0.06253591924905777,\n",
       " 0.07465627044439316,\n",
       " 0.0797128826379776,\n",
       " 0.03989396244287491,\n",
       " 0.11513364315032959,\n",
       " 0.04095321521162987,\n",
       " 0.08559544384479523,\n",
       " 0.07336081564426422,\n",
       " 0.0862964391708374,\n",
       " 0.0635761246085167,\n",
       " 0.06602229923009872,\n",
       " 0.03428197652101517,\n",
       " 0.08080436289310455,\n",
       " 0.026617875322699547,\n",
       " 0.06641169637441635,\n",
       " 0.030969126150012016,\n",
       " 0.06854098290205002,\n",
       " 0.03479662910103798,\n",
       " 0.060931865125894547,\n",
       " 0.04597890377044678,\n",
       " 0.05617627501487732,\n",
       " 0.11842648684978485,\n",
       " 0.046302713453769684,\n",
       " 0.04722905531525612,\n",
       " 0.07113822549581528,\n",
       " 0.06391341239213943,\n",
       " 0.08457724004983902,\n",
       " 0.04908623918890953,\n",
       " 0.05304894596338272,\n",
       " 0.08656104654073715,\n",
       " 0.03454382345080376,\n",
       " 0.09366301447153091,\n",
       " 0.05827854573726654,\n",
       " 0.09353399276733398,\n",
       " 0.051633499562740326,\n",
       " 0.0760471448302269,\n",
       " 0.08368107676506042,\n",
       " 0.06980191916227341,\n",
       " 0.11318646371364594,\n",
       " 0.06906233727931976,\n",
       " 0.06521996110677719,\n",
       " 0.05938291549682617,\n",
       " 0.1410541981458664,\n",
       " 0.08408413082361221,\n",
       " 0.05959179624915123,\n",
       " 0.06283153593540192,\n",
       " 0.05404529348015785,\n",
       " 0.05958663299679756,\n",
       " 0.07740051299333572,\n",
       " 0.047350917011499405,\n",
       " 0.07416751980781555,\n",
       " 0.04475470259785652,\n",
       " 0.03981215879321098,\n",
       " 0.09300240129232407,\n",
       " 0.1156378835439682,\n",
       " 0.07745876908302307,\n",
       " 0.036034855991601944,\n",
       " 0.04429634287953377,\n",
       " 0.049932703375816345,\n",
       " 0.09942073374986649,\n",
       " 0.06253556162118912,\n",
       " 0.05918293818831444,\n",
       " 0.05216401070356369,\n",
       " 0.06756766885519028,\n",
       " 0.031025327742099762,\n",
       " 0.05933007597923279,\n",
       " 0.04232236370444298,\n",
       " 0.06877012550830841,\n",
       " 0.06125036999583244,\n",
       " 0.07253941148519516,\n",
       " 0.08739698678255081,\n",
       " 0.06300894916057587,\n",
       " 0.02055162377655506,\n",
       " 0.06431101262569427,\n",
       " 0.06156127154827118,\n",
       " 0.07105784863233566,\n",
       " 0.03236501291394234,\n",
       " 0.057195574045181274,\n",
       " 0.0576917938888073,\n",
       " 0.034705739468336105,\n",
       " 0.05392345041036606,\n",
       " 0.04904671385884285,\n",
       " 0.07091451436281204,\n",
       " 0.033391401171684265,\n",
       " 0.05526753515005112,\n",
       " 0.04941196367144585,\n",
       " 0.122003473341465,\n",
       " 0.08353517949581146,\n",
       " 0.04947803542017937,\n",
       " 0.060316286981105804,\n",
       " 0.0632249265909195,\n",
       " 0.043137166649103165,\n",
       " 0.07003913074731827,\n",
       " 0.07483047991991043,\n",
       " 0.06822708249092102,\n",
       " 0.04924850910902023,\n",
       " 0.12208948284387589,\n",
       " 0.021095097064971924,\n",
       " 0.06309442222118378,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = prompt.loc[:47000].reset_index(drop=True)\n",
    "val = prompt.loc[47000:].reset_index(drop=True)\n",
    "train.to_csv('./data/recall_train.csv',index=False)\n",
    "val.to_csv('./data/recall_val.csv',index=False)\n",
    "\n",
    "train_loader=get_loader(train,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      train_mode=True,\n",
    "                      num_workers=2)\n",
    "val_loader=get_loader(val, batch_size=BATCH_SIZE,\n",
    "                     train_mode=False,\n",
    "                     num_workers=2)\n",
    "model= RecallModel().to(DEVICE)\n",
    "trainer(train_loader,val_loader,model, \n",
    "            epochs=100, \n",
    "            model_save_path = './save/recall/recall.bin',\n",
    "            accumulation_steps=1,\n",
    "            early_stop_epochs=5, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55551aea-429a-45fb-a1f8-f7ea01a1b038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5895f-8a02-47db-9ef7-48bdd29ffce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100e152-1a3c-4304-b8ef-c122b1bcc007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea420c52-f009-4d00-815f-0cb36fae877d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chr_env",
   "language": "python",
   "name": "chr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
