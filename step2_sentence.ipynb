{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbc3703-f70c-48ac-bb24-353d9043953a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/conda_env/chr_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import collections\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,GroupKFold\n",
    "import torch\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from model import EMA\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from  transformers import AdamW, AutoTokenizer,AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results\n",
    "    Arguments:\n",
    "        seed {int} -- Number of the seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "SEED=2020\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45508728-78a2-41ea-a575-3d730257ca5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_documents(documents: Iterable[str],\n",
    "                      document_ids: Iterable,\n",
    "                      split_sentences: bool = True,\n",
    "                      filter_len: int = 3,\n",
    "                      disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main helper function to process documents from the EMR.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param document_type: String denoting the document type to be processed\n",
    "    :param document_sections: List of sections for a given document type to process\n",
    "    :param split_sentences: Flag to determine whether to further split sections into sentences\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "    \n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values, \n",
    "                        df.document_id.values,\n",
    "                        df.offset.values, \n",
    "                        filter_len, \n",
    "                        disable_progress_bar)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sectionize_documents(documents: Iterable[str],\n",
    "                         document_ids: Iterable,\n",
    "                         disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the \n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row['document_id'] = document_id\n",
    "        row['text'] = text\n",
    "        row['offset'] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def sentencize(documents: Iterable[str],\n",
    "               document_ids: Iterable,\n",
    "               offsets: Iterable[tuple[int, int]],\n",
    "               filter_len: int = 3,\n",
    "               disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param offsets: Iterable tuple of the start and end indices\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n",
    "        try:\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            for o in sentence_offsets:\n",
    "                if o[1]-o[0] > filter_len:\n",
    "                    sentence = document[o[0]:o[1]]\n",
    "                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n",
    "                    row = {}\n",
    "                    row['document_id'] = document_id\n",
    "                    row['text'] = sentence\n",
    "                    row['offset'] = abs_offsets\n",
    "                    document_sentences.append(row)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e94abc-3046-4f18-a295-2dad73f2613d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "BERT_PATH = \"/root/bert_path/sentence-transformer-all-mpnet-base-v2\"\n",
    "MODEL_PATH = \"./save/recall/2023_recall_v1_add_text_nice_valid.pkl\"\n",
    "PROMPT_LEN = 512\n",
    "WIKI_LEN = 512\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 200\n",
    "DEVICE = 'cuda'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f6c3c9-a59d-4ac5-8bd6-32668ee2324b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = pd.read_csv('./data/7w8_crawl_dataset.csv')\n",
    "prompt['prompt_answer'] = prompt.apply(lambda row: ' '.join(str(row[field]) for field in ['prompt', 'A', 'B', 'C', 'D', 'E']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c770a7-eb23-4914-be1c-2adc9aecf52d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "      <th>wiki_text</th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>stem_label</th>\n",
       "      <th>prompt_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>What does the nullity of a graph in graph theo...</td>\n",
       "      <td>The number of vertices in the graph</td>\n",
       "      <td>The number of edges in the graph</td>\n",
       "      <td>The rank of the adjacency matrix</td>\n",
       "      <td>The multiplicity of the eigenvalue 0 in the sp...</td>\n",
       "      <td>The number of components of the graph</td>\n",
       "      <td>D</td>\n",
       "      <td>The nullity of a graph in the mathematical sub...</td>\n",
       "      <td>17458663</td>\n",
       "      <td>Nullity (graph theory)</td>\n",
       "      <td>M</td>\n",
       "      <td>What does the nullity of a graph in graph theo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the matrix theory of graphs, what is the nu...</td>\n",
       "      <td>n − r</td>\n",
       "      <td>m − n + c</td>\n",
       "      <td>n − c</td>\n",
       "      <td>The nullity of the graph</td>\n",
       "      <td>The number of components of the graph</td>\n",
       "      <td>A</td>\n",
       "      <td>The nullity of a graph in the mathematical sub...</td>\n",
       "      <td>17458663</td>\n",
       "      <td>Nullity (graph theory)</td>\n",
       "      <td>M</td>\n",
       "      <td>In the matrix theory of graphs, what is the nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Which term is more commonly used to refer to t...</td>\n",
       "      <td>Nullity of the adjacency matrix</td>\n",
       "      <td>Multiplicity of the eigenvalue 0</td>\n",
       "      <td>Cycle rank</td>\n",
       "      <td>Cyclomatic number</td>\n",
       "      <td>Circuit rank</td>\n",
       "      <td>C</td>\n",
       "      <td>The nullity of a graph in the mathematical sub...</td>\n",
       "      <td>17458663</td>\n",
       "      <td>Nullity (graph theory)</td>\n",
       "      <td>M</td>\n",
       "      <td>Which term is more commonly used to refer to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>What is the formula for calculating the nullit...</td>\n",
       "      <td>n − r</td>\n",
       "      <td>m − n + c</td>\n",
       "      <td>n − c</td>\n",
       "      <td>The nullity of the graph</td>\n",
       "      <td>The number of components of the graph</td>\n",
       "      <td>B</td>\n",
       "      <td>The nullity of a graph in the mathematical sub...</td>\n",
       "      <td>17458663</td>\n",
       "      <td>Nullity (graph theory)</td>\n",
       "      <td>M</td>\n",
       "      <td>What is the formula for calculating the nullit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>What can the nullity of the graph represent in...</td>\n",
       "      <td>The number of vertices in the graph</td>\n",
       "      <td>The number of edges in the graph</td>\n",
       "      <td>The rank of the adjacency matrix</td>\n",
       "      <td>The number of components of the graph</td>\n",
       "      <td>The rank of the oriented incidence matrix</td>\n",
       "      <td>E</td>\n",
       "      <td>The nullity of a graph in the mathematical sub...</td>\n",
       "      <td>17458663</td>\n",
       "      <td>Nullity (graph theory)</td>\n",
       "      <td>M</td>\n",
       "      <td>What can the nullity of the graph represent in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                             prompt  \\\n",
       "0           0   0  What does the nullity of a graph in graph theo...   \n",
       "1           1   1  In the matrix theory of graphs, what is the nu...   \n",
       "2           2   2  Which term is more commonly used to refer to t...   \n",
       "3           3   3  What is the formula for calculating the nullit...   \n",
       "4           4   4  What can the nullity of the graph represent in...   \n",
       "\n",
       "                                     A                                 B  \\\n",
       "0  The number of vertices in the graph  The number of edges in the graph   \n",
       "1                                n − r                         m − n + c   \n",
       "2      Nullity of the adjacency matrix  Multiplicity of the eigenvalue 0   \n",
       "3                                n − r                         m − n + c   \n",
       "4  The number of vertices in the graph  The number of edges in the graph   \n",
       "\n",
       "                                  C  \\\n",
       "0  The rank of the adjacency matrix   \n",
       "1                             n − c   \n",
       "2                        Cycle rank   \n",
       "3                             n − c   \n",
       "4  The rank of the adjacency matrix   \n",
       "\n",
       "                                                   D  \\\n",
       "0  The multiplicity of the eigenvalue 0 in the sp...   \n",
       "1                           The nullity of the graph   \n",
       "2                                  Cyclomatic number   \n",
       "3                           The nullity of the graph   \n",
       "4              The number of components of the graph   \n",
       "\n",
       "                                           E answer  \\\n",
       "0      The number of components of the graph      D   \n",
       "1      The number of components of the graph      A   \n",
       "2                               Circuit rank      C   \n",
       "3      The number of components of the graph      B   \n",
       "4  The rank of the oriented incidence matrix      E   \n",
       "\n",
       "                                           wiki_text   page_id  \\\n",
       "0  The nullity of a graph in the mathematical sub...  17458663   \n",
       "1  The nullity of a graph in the mathematical sub...  17458663   \n",
       "2  The nullity of a graph in the mathematical sub...  17458663   \n",
       "3  The nullity of a graph in the mathematical sub...  17458663   \n",
       "4  The nullity of a graph in the mathematical sub...  17458663   \n",
       "\n",
       "               page_title stem_label  \\\n",
       "0  Nullity (graph theory)          M   \n",
       "1  Nullity (graph theory)          M   \n",
       "2  Nullity (graph theory)          M   \n",
       "3  Nullity (graph theory)          M   \n",
       "4  Nullity (graph theory)          M   \n",
       "\n",
       "                                       prompt_answer  \n",
       "0  What does the nullity of a graph in graph theo...  \n",
       "1  In the matrix theory of graphs, what is the nu...  \n",
       "2  Which term is more commonly used to refer to t...  \n",
       "3  What is the formula for calculating the nullit...  \n",
       "4  What can the nullity of the graph represent in...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca5a292c-5386-48dc-af1b-d77dd2f78b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78008/78008 [00:00<00:00, 195044.37it/s]\n",
      "100%|██████████| 78008/78008 [01:04<00:00, 1202.07it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_wiki_text_data = process_documents(prompt.wiki_text.values, prompt.id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e284c4-3b47-4b3f-9039-5252a2fc385c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78008/78008 [02:34<00:00, 503.54it/s]\n"
     ]
    }
   ],
   "source": [
    "splits = []\n",
    "for i in tqdm(range(len(prompt))):\n",
    "    prompt.loc[i,'answer'] = prompt.loc[i, prompt.loc[i,'answer']]\n",
    "    splits.append(processed_wiki_text_data[processed_wiki_text_data['document_id'] == i]['text'].tolist())\n",
    "prompt['sentence'] = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "013feec4-4f07-4358-a882-47db8e88c9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt[prompt['sentence'].apply(lambda x : len(x) != 0)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f15dff-70b9-492e-b923-d02f2a88c096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77872, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2d5540-2893-43a7-b9d0-bdb9d96e32b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pylcs\n",
    "def rouge_l(a, b):\n",
    "    if b == ' ':\n",
    "        return 0\n",
    "    lcs = pylcs.lcs(a, b)\n",
    "    r = lcs / len(a)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd150b6-e7a0-4055-bc49-d8d8ee4274e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77872/77872 [01:13<00:00, 1055.38it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "s = set()\n",
    "for i in tqdm(range(len(prompt))):\n",
    "    choices = [x for x in prompt.loc[i,'sentence'] if len(x) > 0]\n",
    "    if len(choices) == 0:\n",
    "        sentence.append(' ')\n",
    "        continue\n",
    "    for item in choices:\n",
    "        s.add(item)\n",
    "    scores = []\n",
    "    for item in choices:\n",
    "        scores.append(rouge_l(prompt.loc[i,'answer'].lower(), item))\n",
    "    sentence.append(choices[scores.index(max(scores))])\n",
    "sentence_df = pd.DataFrame({'sentence_answer':list(s)})\n",
    "prompt['sentence_answer'] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40ed50a8-5a87-4e98-aaa0-a93f12edf082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLMRecallDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(BERT_PATH, use_fast=True)\n",
    "        self.query = []\n",
    "        self.answer = []\n",
    "        print('加载数据集中')\n",
    "        for i in tqdm(range(len(data))):\n",
    "            query = data.loc[i, 'prompt_answer']\n",
    "            answer = data.loc[i, 'sentence_answer']\n",
    "            query_id = self.tokenizer.encode(query, add_special_tokens=False)\n",
    "            answer_id = self.tokenizer.encode(answer, add_special_tokens=False)\n",
    "            if len(query_id) > 510:\n",
    "                query_id = [101] + query_id[:510] + [102]\n",
    "            else:\n",
    "                query_id = [101] + query_id + [102]\n",
    "            if len(answer_id) > 510:\n",
    "                answer_id = [101] + answer_id[:510] + [102]\n",
    "            else:\n",
    "                answer_id = [101] + answer_id + [102]\n",
    "            self.query.append(query_id)\n",
    "            self.answer.append(answer_id)\n",
    "    def __len__(self):\n",
    "        return len(self.query) \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.query[index], self.answer[index]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        def sequence_padding(inputs, length=None, padding=0):\n",
    "            \"\"\"\n",
    "            Numpy函数，将序列padding到同一长度\n",
    "            \"\"\"\n",
    "            if length is None:\n",
    "                length = max([len(x) for x in inputs])\n",
    "\n",
    "            pad_width = [(0, 0) for _ in np.shape(inputs[0])]\n",
    "            outputs = []\n",
    "            for x in inputs:\n",
    "                x = x[:length]\n",
    "                pad_width[0] = (0, length - len(x))\n",
    "                x = np.pad(x, pad_width, 'constant', constant_values=padding)\n",
    "                outputs.append(x)\n",
    "\n",
    "            return np.array(outputs, dtype='int64')\n",
    "        batch_query, batch_answer = [], []\n",
    "        \n",
    "        for item in batch:\n",
    "            query, answer = item\n",
    "            batch_query.append(query)\n",
    "            batch_answer.append(answer)\n",
    "        batch_query = torch.tensor(sequence_padding(batch_query), dtype=torch.long)\n",
    "        batch_answer = torch.tensor(sequence_padding(batch_answer), dtype=torch.long)\n",
    "        \n",
    "        return batch_query, batch_answer\n",
    "\n",
    "        \n",
    "class DataLoaderX(torch.utils.data.DataLoader):\n",
    "    '''\n",
    "        replace DataLoader with PrefetchDataLoader\n",
    "    '''\n",
    "    def __iter__(self):\n",
    "        return BackgroundGenerator(super().__iter__())  \n",
    "\n",
    "    \n",
    "def get_loader(prompt,batch_size,train_mode=True,num_workers=4):\n",
    "    ds_df = LLMRecallDataSet(prompt)\n",
    "    # loader = DataLoaderX(ds_df, batch_size=batch_size if train_mode else batch_size // 2, shuffle=train_mode, num_workers=num_workers,\n",
    "    #                      pin_memory=True,\n",
    "    #                      collate_fn=ds_df.collate_fn, drop_last=train_mode)\n",
    "    dataloader_class = partial(DataLoader, pin_memory=True)\n",
    "    loader = dataloader_class(ds_df, batch_size=batch_size, shuffle=train_mode, collate_fn=ds_df.collate_fn, num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "def debug_loader(prompt, batch_size):\n",
    "    loader=get_loader(prompt,batch_size,train_mode=True,num_workers=0)\n",
    "    for token_ids,labels in loader:\n",
    "        print(token_ids)\n",
    "        print(labels)\n",
    "        break\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c2b3c30-515c-4d8d-8f6e-5f2aa7531372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class RecallModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecallModel, self).__init__()\n",
    "        self.bert_model = AutoModel.from_pretrained(BERT_PATH)\n",
    "        # self.linear = nn.Linear(768, 384)\n",
    "        self.mean_pooler = MeanPooling()\n",
    "\n",
    "    def mask_mean(self, x, mask=None):\n",
    "        if mask != None:\n",
    "            mask_x = x * (mask.unsqueeze(-1))\n",
    "            x_sum = torch.sum(mask_x, dim=1)\n",
    "            re_x = torch.div(x_sum, torch.sum(mask, dim=1).unsqueeze(-1))\n",
    "        else:\n",
    "            x_sum = torch.sum(x, dim=1)\n",
    "            re_x = torch.div(x_sum, x.size()[1])\n",
    "        return re_x\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        attention_mask = input_ids > 0\n",
    "        out = self.bert_model(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        # out = self.linear(out)\n",
    "        x = self.mean_pooler(out, attention_mask)\n",
    "\n",
    "        # x = out[:, 0, :]\n",
    "        return x\n",
    "\n",
    "def debug_label():\n",
    "    loader=get_loader(prompt,batch_size=2,train_mode=True,num_workers=0)\n",
    "    model= RecallModel()\n",
    "    print('models paramters:', sum(p.numel() for p in model.parameters()))\n",
    "    for token_ids,labels in loader:\n",
    "        # print(token_ids)\n",
    "        # print(labels)\n",
    "        prob=model(token_ids)\n",
    "        print(prob)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99f069a5-5a31-45e5-b13c-c91f8f5e1da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SimCSE_loss(topic_pred,content_pred,tau=0.05):\n",
    "    similarities = F.cosine_similarity(topic_pred.unsqueeze(1), content_pred.unsqueeze(0), dim=2) # B,B\n",
    "    y_true = torch.arange(0,topic_pred.size(0)).to(DEVICE)\n",
    "    # similarities = similarities - torch.eye(pred.shape[0]) * 1e12\n",
    "    similarities = similarities / tau\n",
    "    loss=F.cross_entropy(similarities, y_true)\n",
    "    return torch.mean(loss)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import faiss\n",
    "def valid(model):\n",
    "    model.eval()\n",
    "    index = faiss.IndexFlatIP(768)\n",
    "    prompt_embed = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            topic_inputs, content_inputs = (_.to(DEVICE) for _ in batch)\n",
    "            with autocast():\n",
    "                topic_pred = model(topic_inputs).cpu().numpy()\n",
    "            faiss.normalize_L2(topic_pred)\n",
    "            prompt_embed.append(topic_pred)\n",
    "        for batch in tqdm(sentence_loader):\n",
    "            topic_inputs, content_inputs = (_.to(DEVICE) for _ in batch)\n",
    "            with autocast():\n",
    "                content_pred = model(content_inputs).cpu().numpy()\n",
    "            faiss.normalize_L2(content_pred)\n",
    "            index.add(content_pred)\n",
    "    prompt_embed = np.concatenate(prompt_embed, axis=0)\n",
    "    search_score, search_index = index.search(prompt_embed, 1)\n",
    "    cnt = 0\n",
    "    for i in range(len(val)):\n",
    "        label = val.loc[i, 'sentence_answer']\n",
    "        pred = sentence_df.loc[search_index[i][0],'sentence_answer']\n",
    "        if label == pred:\n",
    "            cnt += 1\n",
    "        \n",
    "    return cnt / len(val)\n",
    "\n",
    "def trainer(train_dataloader,val_dataloader,model, epochs, fold=0,\n",
    "            accumulation_steps=1, early_stop_epochs=5, device='cpu'):\n",
    "    ########早停\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    ########优化器 学习率\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    crf_p = [n for n, p in param_optimizer if str(n).find('crf') != -1]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and n not in crf_p], 'weight_decay': 1e-6},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and n not in crf_p], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in param_optimizer if n in crf_p], 'lr': 2e-3, 'weight_decay': 0.8},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_len = len(train_dataloader)\n",
    "\n",
    "    ema_inst = EMA(model, 0.95)\n",
    "    ema_inst.register()\n",
    "\n",
    "    best_score = 0\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        bar = tqdm(train_dataloader)\n",
    "        for i, inputs in enumerate(bar):\n",
    "            with autocast():\n",
    "                topic_inputs, content_inputs = (_.to(device) for _ in inputs)\n",
    "                # print(topic_inputs.size())\n",
    "                # print(content_inputs.size())\n",
    "                topic_pred = model(topic_inputs)\n",
    "                content_pred = model(content_inputs)\n",
    "                # print(topic_pred.size())\n",
    "                # print(content_pred.size())\n",
    "                loss = SimCSE_loss(topic_pred, content_pred)\n",
    "            scaler.scale(loss).backward()\n",
    "            losses.append(loss.item())\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == train_len:\n",
    "                scaler.step(optimizer)\n",
    "                if ema_inst:\n",
    "                    ema_inst.update()\n",
    "                optimizer.zero_grad()\n",
    "                scaler.update()\n",
    "            bar.set_postfix(loss_mean=np.array(losses).mean(), epoch=epoch)\n",
    "\n",
    "        if ema_inst:\n",
    "            ema_inst.apply_shadow()\n",
    "        score = valid(model)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(), f'./save/recall_sentence/recall_best.bin')\n",
    "        print(f'score:{score} best_score:{best_score}')\n",
    "        if ema_inst:\n",
    "            ema_inst.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "108a6e5f-d072-46ff-9d0a-c9eff7de7a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据集中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 15158/127332 [00:05<00:38, 2894.24it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 127332/127332 [00:45<00:00, 2813.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据集中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4504/62297 [00:03<00:40, 1410.88it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 62297/62297 [00:43<00:00, 1438.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据集中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 6962/15575 [00:04<00:05, 1609.60it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 15575/15575 [00:10<00:00, 1546.52it/s]\n",
      "/root/conda_env/chr_env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s, epoch=1, loss_mean=5.04]\n",
      "100%|██████████| 637/637 [02:44<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.024462279293739966 best_score:0.024462279293739966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [03:50<00:00,  1.35it/s, epoch=2, loss_mean=4.9] \n",
      "100%|██████████| 637/637 [02:36<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.026902086677367577 best_score:0.026902086677367577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 21/312 [00:15<03:35,  1.35it/s, epoch=3, loss_mean=4.88]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m=\u001b[39m RecallModel()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39mDataParallel(model)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stop_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 73\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(train_dataloader, val_dataloader, model, epochs, fold, accumulation_steps, early_stop_epochs, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# print(topic_inputs.size())\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(content_inputs.size())\u001b[39;00m\n\u001b[1;32m     72\u001b[0m topic_pred \u001b[38;5;241m=\u001b[39m model(topic_inputs)\n\u001b[0;32m---> 73\u001b[0m content_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# print(topic_pred.size())\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# print(content_pred.size())\u001b[39;00m\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m SimCSE_loss(topic_pred, content_pred)\n",
      "File \u001b[0;32m~/conda_env/chr_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/conda_env/chr_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/conda_env/chr_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda_env/chr_env/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:81\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     79\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 81\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/conda_env/chr_env/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/conda_env/chr_env/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "sentence_df['prompt_answer'] = 'A'\n",
    "sentence_loader = get_loader(sentence_df, batch_size=BATCH_SIZE,\n",
    "                         train_mode=False,\n",
    "                         num_workers=0)\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(prompt['prompt_answer'].tolist(), prompt['sentence_answer'].tolist(), prompt['page_id'].tolist())):\n",
    "    train = prompt.loc[train_idx].reset_index(drop=True)\n",
    "    val = prompt.loc[val_idx].reset_index(drop=True)\n",
    "    train_loader=get_loader(train,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          train_mode=True,\n",
    "                          num_workers=0)\n",
    "    val_loader=get_loader(val, batch_size=BATCH_SIZE,\n",
    "                         train_mode=False,\n",
    "                         num_workers=0)\n",
    "    model= RecallModel().to(DEVICE)\n",
    "    model = torch.nn.parallel.DataParallel(model)\n",
    "    trainer(train_loader,val_loader,model,\n",
    "                epochs=20,\n",
    "                fold = fold,\n",
    "                accumulation_steps=1,\n",
    "                early_stop_epochs=5, device=DEVICE)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e959b0-5c69-460f-bc38-b3fbe0b19f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 390/390 [00:14<00:00, 27.19it/s]\n",
      "Batches: 100%|██████████| 637/637 [00:16<00:00, 37.73it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('/root/bert_path/sentence-transformers_all-MiniLM-L6-v2', device='cuda')\n",
    "model.max_seq_length = 512\n",
    "model = model.half()\n",
    "## Get embeddings of the wiki text data\n",
    "prompt_embed = model.encode(prompt.prompt_answer,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    device=DEVICE,\n",
    "                                    show_progress_bar=True,\n",
    "                                    convert_to_tensor=True,\n",
    "                                    normalize_embeddings=True)#.half()\n",
    "prompt_embed = prompt_embed.detach().cpu().numpy()\n",
    "page_embed = model.encode(sentence_df.sentence_answer,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    device=DEVICE,\n",
    "                                    show_progress_bar=True,\n",
    "                                    convert_to_tensor=True,\n",
    "                                    normalize_embeddings=True)#.half()\n",
    "page_embed = page_embed.detach().cpu().numpy()\n",
    "index = faiss.IndexFlatIP(384)\n",
    "index.add(page_embed)\n",
    "_,search_index = index.search(prompt_embed, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dda8f64-d986-4370-9035-9e6e93a09c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77872/77872 [00:04<00:00, 18973.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.035622560098623385"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in tqdm(range(len(prompt))):\n",
    "    label = prompt.loc[i, 'sentence_answer']\n",
    "    pred = sentence_df.loc[search_index[i][0]]['sentence_answer']\n",
    "    if label in pred:\n",
    "        cnt += 1\n",
    "cnt / len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6f882-4a49-4c01-ab9f-55f44929a4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chr_env",
   "language": "python",
   "name": "chr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
