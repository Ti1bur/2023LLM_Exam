{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74e246d-0189-4de8-b3f5-7c3c587fa8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/conda_env/chr_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28064ef-26cc-48ff-a89a-ddf0e103bbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.parquet\n",
      "b.parquet\n",
      "c.parquet\n",
      "d.parquet\n",
      "e.parquet\n",
      "f.parquet\n",
      "g.parquet\n",
      "h.parquet\n",
      "i.parquet\n",
      "j.parquet\n"
     ]
    }
   ],
   "source": [
    "### load data \n",
    "import os\n",
    "tmp = []\n",
    "list_dir = os.listdir('./wiki_data')\n",
    "list_dir = sorted(list_dir)\n",
    "for path in list_dir:\n",
    "    if 'wiki_2023_index' in path or 'ipynb' in path or 'my' in path:\n",
    "        continue\n",
    "    print(path)\n",
    "    x = pd.read_parquet(os.path.join('./wiki_data',path))\n",
    "    x['file'] = path\n",
    "    tmp.append(x)\n",
    "doc = pd.concat(tmp, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2c61d-e729-421f-8a39-375dff9e51f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc['context'] = doc['title']+\" \"+doc['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844ffc20-b2e2-44e4-9f96-6727d217c745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def word_process(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "    return words\n",
    "    \n",
    "\n",
    "corpus = list(doc['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f03ec8-04d9-41b8-b860-f13a08d6ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "def process(data,idx):\n",
    "    res = []\n",
    "    if idx == 0:\n",
    "        for item in tqdm(data):\n",
    "            res.append(word_process(item))\n",
    "    else:\n",
    "        for item in data:\n",
    "            res.append(word_process(item))\n",
    "    import pickle as pkl\n",
    "    with open(f'./tmp/corpus_{idx}.pkl','wb') as f:\n",
    "        pkl.dump(res, f)\n",
    "job = 60\n",
    "sub_size = len(corpus) // job\n",
    "remain = len(corpus) % job\n",
    "subs = [corpus[i:i+sub_size] for i in range(0, len(corpus)-remain, sub_size)]\n",
    "if remain:\n",
    "    subs[-1].extend(corpus[-remain:])\n",
    "process = [multiprocessing.Process(target=process, args=(subs[i], i)) for i in range(job)]\n",
    "for pro in process:\n",
    "    pro.start()\n",
    "for pro in process:\n",
    "    pro.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbe92df-a52a-462e-9265-863d910e7eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [11:37<00:00, 11.62s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "job = 60\n",
    "for item in tqdm(range(job)):\n",
    "    with open(f'./tmp/corpus_{item}.pkl','rb') as f:\n",
    "        tokenized_corpus.extend(pkl.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a089712-d1ef-48bb-a70f-50fe86e869f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3407835861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in tokenized_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e3e5990-0ed1-48f1-895d-a958c6420089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(tokenized_corpus, vector_size=384, sg=1, epochs=10, min_count=5, window=32, dtype=np.float32, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0c52fd8-1b67-411e-a3d0-832aef1eff25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('./save/w2v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cdd243-1fe3-4d5c-97e8-12eb0349d7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"./save/w2v.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94aba30-d6cb-4865-a3c1-22a10e0b742f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 4627172/6286775 [3:43:56<1:08:33, 403.50it/s] "
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "index = faiss.IndexFlatIP(384)\n",
    "for item in tqdm(tokenized_corpus):\n",
    "    vec = [model.wv[word] for word in item if word in model.wv]\n",
    "    if len(vec) == 0:\n",
    "        output = np.full([384],10000, dtype=np.float32)\n",
    "    else:\n",
    "        output = sum(vec) / len(vec)\n",
    "    output = output[None, :]\n",
    "    output = output.astype(np.float32)\n",
    "    faiss.normalize_L2(output)\n",
    "    index.add(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7abbd89-be4c-4495-b584-44d301f5b932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faiss.write_index(index, './wiki_index/w2v_index.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcfa5f-93cd-4121-9cd6-bdd76eafed17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chr_env",
   "language": "python",
   "name": "chr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
